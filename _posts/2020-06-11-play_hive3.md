---
layout: blog
title: "Play with Hive 3"
---

I've been working for a while in the company that provides data analytics platform and enjoying interesting 
storage layer problems. Although I may roughly understand the whole picture of our Presto / Hive based
MPP architecture, I recently feels that I need deeper understanding of their internal behavior to work on
complicated performance problems.

Built and played with the latest Hive and Hadoop as study.
<!--end_excerpt-->

# Setup
Goal is to execute queries by hive on top of HDFS and MapReduce / Tez. I also think to use this environment to 
run Presto in the next step.

Note: I setup my enviroment just as a sandbox so security, reliability and availability are not considered well.

## Environment and Software Versions
### Hardware
```
$ lscpu
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   39 bits physical, 48 bits virtual
CPU(s):                          6
On-line CPU(s) list:             0-5
Thread(s) per core:              1
Core(s) per socket:              6
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           158
Model name:                      Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz
Stepping:                        10
CPU MHz:                         2394.513
CPU max MHz:                     4100.0000
CPU min MHz:                     800.0000
BogoMIPS:                        6000.00
Virtualization:                  VT-x
L1d cache:                       192 KiB
L1i cache:                       192 KiB
L2 cache:                        1.5 MiB
L3 cache:                        9 MiB
NUMA node0 CPU(s):               0-5
```

All processes run on single server.

### Software
```
$ uname -srvmo
Linux 5.4.0-29-generic #33-Ubuntu SMP Wed Apr 29 14:32:27 UTC 2020 x86_64 GNU/Linux

$ java -version
openjdk version "1.8.0_252"
OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1ubuntu1-b09)
OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)

$ ./bin/hadoop version
Hadoop 3.2.1
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842
Compiled by rohithsharmaks on 2019-09-10T15:56Z
Compiled with protoc 2.5.0
From source with checksum 776eaf9eee9c0ffc370bcbc1888737
```

Hadoop run by [pseudo-distributed mode](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation) .

- Hive 3.1.2
  - replaced guava jar bundled on hive with the one of Hadoop as a workaround of [guava version incompatibility problem](https://issues.apache.org/jira/browse/HIVE-22126)
- PostgreSQL 12.3 (for metastore)
```
$ docker run --name pg-12 -e POSTGRES_PASSWORD= -e POSTGRES_HOST_AUTH_METHOD=trust -d postgres:12.3
```

## Setup HDFS & YARN
Follow steps of [pseudo-distributed mode](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operatio)

+ Install dependencies
```
# apt-get install ssh pdsh
```
+ Check ssh to the localhost without password
+ Get hadoop distribution
+ Edit configurations
  - etc/hadoop/core-site.xml

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```
  - etc/hadoop/hdfs-site.xml

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```
+ Format HDFS
```
$ $HADOOP_HOME/bin/hdfs namenode -format
$ $HADOOP_HOME/sbin/start-dfs.sh
$ $HADOOP_HOME/sbin/start-yarn.sh
```

Note: HDFS data dir (dfs.datanode.data.dir) is `/tmp/hadoop-${user.name}/dfs/data` so it disappears by reboot. 

## Setup Hive
Follow https://cwiki.apache.org/confluence/display/Hive/GettingStarted .

+ Get Hive distribution and extract
+ Set `HADOOP_HOME` and `HIVE_HOME`
+ Create directories to store Hive tables
```
$ $HADOOP_HOME/bin/hadoop fs -mkdir /tmp
$ $HADOOP_HOME/bin/hadoop fs -mkdir -p /user/hive/warehouse  # hive.metastore.warehouse.dir
$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp
$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse
```
+ Edit configure to use PostgreSQL for metastore
  - hive-site.xml (hive-default.xml didn't work for schematool)

```
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://${HOST}:${PORT}/hive_metastore</value>
        <description>
            JDBC connect string for a JDBC metastore.
            To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
            For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
        </description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>postgres</value>
        <description>Username to use against metastore database</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value></value>
        <description>password to use against metastore database</description>
    </property>

    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
        <description>
            Setting this property to true will have HiveServer2 execute
            Hive operations as the user making the calls to it.
        </description>
    </property>
</configuration>
```
+ Initialize metastore schema
```
$ $HIVE_HOME/bin/schematool -dbType postgres -initSchema
```
+ Boot hiveserver2
```
$ $HIVE_HOME/bin/hiveserver2
```

## Connect Hiveserver
```
$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000
```

Note: Bootstrap of hiveserver may take some time.
Now it's ready to execute queries.

# Play with Queries
## Simple insert
- Beeline
```
> CREATE TABLE test (id int, value int);
...
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=...); Time taken: 0.056 seconds
...
> INSERT INTO insert into test values (1, 1), (2, 2), (3, 3);
...
INFO  : Total jobs = 3
INFO  : Launching Job 1 out of 3
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1591875950345_0001
INFO  : Executing with tokens: []
...
INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
INFO  : 2020-06-11 23:00:23,692 Stage-1 map = 0%,  reduce = 0%
INFO  : 2020-06-11 23:00:27,818 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.02 sec
INFO  : 2020-06-11 23:00:32,923 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.19 sec
INFO  : MapReduce Total cumulative CPU time: 3 seconds 190 msec
INFO  : Ended Job = job_1591875950345_0002
INFO  : Starting task [Stage-7:CONDITIONAL] in serial mode
INFO  : Stage-4 is selected by condition resolver.
INFO  : Stage-3 is filtered out by condition resolver.
INFO  : Stage-5 is filtered out by condition resolver.
INFO  : Starting task [Stage-4:MOVE] in serial mode
INFO  : Moving data to directory hdfs://localhost:9000/user/hive/warehouse/test/.hive-staging_hive_2020-06-11_23-00-19_051_1627075465863944744-2/-ext-10000 from hdfs://localhost:9000/user/hive/warehouse/test/.hive-staging_hive_2020-06-11_23-00-19_051_1627075465863944744-2/-ext-10002
INFO  : Starting task [Stage-0:MOVE] in serial mode
INFO  : Loading data to table default.test from hdfs://localhost:9000/user/hive/warehouse/test/.hive-staging_hive_2020-06-11_23-00-19_051_1627075465863944744-2/-ext-10000
INFO  : Starting task [Stage-2:STATS] in serial mode
INFO  : MapReduce Jobs Launched:
INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.19 sec   HDFS Read: 14302 HDFS Write: 263 SUCCESS
INFO  : Total MapReduce CPU Time Spent: 3 seconds 190 msec
INFO  : Completed executing command(queryId=...); Time taken: 14.843 seconds
```
- File format
```
$ ./bin/hadoop fs -ls /user/hive/warehouse/test
Found 1 items
-rw-r--r--   1 ... supergroup         12 2020-06-11 23:00 /user/hive/warehouse/test/000000_0
$ ./bin/hadoop fs -get /user/hive/warehouse/test/000000_0 .
$ od -Ax -tx1z -v 000000_0
000000 31 01 31 0a 32 01 32 0a 33 01 33 0a              >1.1.2.2.3.3.<
00000c
$ od -t c 000000_0
0000000   1 001   1  \n   2 001   2  \n   3 001   3  \n
0000014
```
